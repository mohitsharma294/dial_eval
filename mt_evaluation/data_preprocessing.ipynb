{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f55a623",
   "metadata": {},
   "source": [
    "# use this script to prepare the dataset offline, once prepared load to GPU server and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce472ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required libraries\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e4b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import  AutoTokenizer\n",
    "import collections\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "import yake\n",
    "import spacy\n",
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4011d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.tsv\", delimiter = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ed06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {\"German-de\": \"German\"})\n",
    "df =df.dropna()\n",
    "df['English'] = df['English'].apply(lambda x:x.strip())\n",
    "df['German'] = df['German'].apply(lambda x:x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43ee6c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to parquet file, easy to load with dataset library\n",
    "df.to_parquet(\"train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f19aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"parquet\", data_files = \"train.parquet\")\n",
    "dataset = dataset.remove_columns('__index_level_0__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be449784",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train'].train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ed4a4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['English', 'German'],\n",
      "        num_rows: 1754716\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['English', 'German'],\n",
      "        num_rows: 194969\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "002e05b2-d42a-4a9c-89c9-52f66def96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"xlm-roberta-base\"\n",
    "# tokenizing english and other text to half to max_len\n",
    "max_len_english  = 64\n",
    "max_len=128\n",
    "mask_random =  False\n",
    "m_ratio = 0.15\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63071d17-f003-4ef0-ae81-b1f80134f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(txt, tokenizer):\n",
    "    \"\"\"\n",
    "    Sentence tokenizer\n",
    "    \"\"\"\n",
    "    result = tokenizer(txt, max_length=max_len_english, padding='max_length', truncation=True)\n",
    "    word_ids = result.word_ids()\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [word_ids[i] for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "def get_word_mapping(tok):\n",
    "    \"\"\"\n",
    "    once sentence is converted into token, this function maps the word id to token id\n",
    "    \"\"\"\n",
    "    word_ids = tok[\"word_ids\"].copy()\n",
    "    mapping = collections.defaultdict(list)\n",
    "    current_word_index = -1\n",
    "    current_word = None\n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is not None:\n",
    "            if word_id != current_word:\n",
    "                current_word = word_id\n",
    "                current_word_index += 1\n",
    "            mapping[current_word_index].append(idx)\n",
    "    return mapping\n",
    "\n",
    "def get_pos_tags(doc):\n",
    "    \"\"\"\n",
    "    From the sentence we get the POS tags, used in masking\n",
    "    \"\"\"\n",
    "    pos_tags = {}\n",
    "    for token in doc:\n",
    "        if(not (token.is_stop or token.is_punct or token.is_space or token.text.lower() in stop_words)):\n",
    "            if(token.tag_ in lst_pos_tags):\n",
    "                pos_tags[token.text] = token.tag_\n",
    "    return pos_tags\n",
    "\n",
    "def get_mask_phrases(txt, tok, mapping, add_pos):\n",
    "    \"\"\"\n",
    "    This function mask the phrases from the sentence\n",
    "    \"\"\"\n",
    "    prev_word = None\n",
    "    prev_id = None\n",
    "    next = False\n",
    "    if(mask_random):\n",
    "        n_sample = math.ceil(0.15*len(mapping))\n",
    "        mask = random.sample(range(len(mapping)),n_sample)\n",
    "        mask_words = []\n",
    "        for idx in mask:\n",
    "            start, end = tok.word_to_chars(idx)\n",
    "            word = txt[start:end].lower()\n",
    "            mask_words.append(word)\n",
    "    else:\n",
    "        yake_doc = txt.replace(tokenizer.eos_token, \"\")\n",
    "        yake_doc = yake_doc.replace(tokenizer.bos_token, \"\")\n",
    "        yake_doc = yake_doc.strip()\n",
    "        max_keyword = max(3, math.ceil(m_ratio*len(mapping)))\n",
    "        keywords = custom_kw_extractor.extract_keywords(yake_doc)[:max_keyword]\n",
    "        lst_kw = [kw[0].lower() for kw in keywords]\n",
    "        if(len(lst_kw)<max_keyword and add_pos):\n",
    "            n = max_keyword-len(lst_kw)\n",
    "            txt_doc = nlp(txt)\n",
    "            pos_tags = get_pos_tags(txt_doc)\n",
    "            for w in pos_tags:\n",
    "                if(w not in lst_kw):\n",
    "                    lst_kw.append(w.lower())\n",
    "                    n = n-1\n",
    "                    if(n==0):\n",
    "                        break\n",
    "\n",
    "        mask = []\n",
    "        mask_words = []\n",
    "        for idx in mapping:\n",
    "            start, end = tok.word_to_chars(idx)\n",
    "            word = txt[start:end].lower()\n",
    "            if word in lst_kw or next:\n",
    "                if prev_word is not None:\n",
    "                    mask.append(prev_id)\n",
    "                    mask_words.append(prev_word)\n",
    "                    mask.append(idx)\n",
    "                    mask_words.append(word)\n",
    "                    prev_word = None\n",
    "                else:\n",
    "                    mask.append(idx)\n",
    "                    mask_words.append(word)\n",
    "                    prev_word = None\n",
    "                if word in lst_kw:\n",
    "                    next = True\n",
    "                else:\n",
    "                    next = False\n",
    "            else:\n",
    "                prev_word = word\n",
    "                prev_id = idx\n",
    "                next = False\n",
    "    return mask, mask_words\n",
    "\n",
    "\n",
    "def get_mask_words(txt, tok, mapping, add_pos):\n",
    "    \"\"\"\n",
    "    This function mask the words from the sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    if(mask_random):\n",
    "        n_sample = math.ceil(0.15*len(mapping))\n",
    "        mask = random.sample(range(len(mapping)),n_sample)\n",
    "        mask_words = []\n",
    "        for idx in mask:\n",
    "            start, end = tok.word_to_chars(idx)\n",
    "            word = txt[start:end].lower()\n",
    "            mask_words.append(word)\n",
    "    else:\n",
    "        yake_doc = txt.replace(tokenizer.eos_token, \"\")\n",
    "        yake_doc = yake_doc.replace(tokenizer.bos_token, \"\")\n",
    "        yake_doc = yake_doc.strip()\n",
    "        max_keyword = max(3, math.ceil(m_ratio*len(mapping)))\n",
    "        keywords = custom_kw_extractor.extract_keywords(yake_doc)[:max_keyword]\n",
    "        lst_kw = [kw[0].lower() for kw in keywords]\n",
    "        if(len(lst_kw)<max_keyword and add_pos):\n",
    "            n = max_keyword-len(lst_kw)\n",
    "            txt_doc = nlp(txt)\n",
    "            pos_tags = get_pos_tags(txt_doc)\n",
    "            for w in pos_tags:\n",
    "                if(w not in lst_kw):\n",
    "                    #lst_kw.append(w)\n",
    "                    lst_kw.append(w.lower())\n",
    "                    n = n-1\n",
    "                    if(n==0):\n",
    "                        break\n",
    "\n",
    "        mask = []\n",
    "        mask_words = []\n",
    "        for idx in mapping:\n",
    "            start, end = tok.word_to_chars(idx)\n",
    "            word = txt[start:end].lower()\n",
    "            if word in lst_kw:\n",
    "                mask.append(idx)\n",
    "                mask_words.append(word)\n",
    "    return mask, mask_words\n",
    "\n",
    "def get_masked_tokens(tokenizer, tok, mapping, mask):\n",
    "    \"\"\"\n",
    "    once we get the mask word id,this function replace with masked tokens\n",
    "    \"\"\"\n",
    "    input_ids = tok[\"input_ids\"].copy()\n",
    "    labels = [-100]*len(input_ids)\n",
    "    for word_id in mask:\n",
    "        for idx in mapping[word_id]:\n",
    "            labels[idx] = input_ids[idx]\n",
    "            input_ids[idx] = tokenizer.mask_token_id\n",
    "    return input_ids, labels\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"\n",
    "    helper function to collate function, to prepare the features i.e. input_ids, lablel\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    english = df['English']\n",
    "    german = df['German']\n",
    "    tok_english = tokenize_sentence(english, tokenizer)\n",
    "    map_english_words = get_word_mapping(tok_english)\n",
    "    mask, mask_words = get_mask_words(english, tok_english, map_english_words, False)\n",
    "    english_masked, label = get_masked_tokens(tokenizer, tok_english, map_english_words, mask)\n",
    "    tok_german = tokenize_sentence(german, tokenizer)\n",
    "    german_labels = [-100]*len(tok_german['input_ids'])\n",
    "    out[\"input_ids\"] = tok_german['input_ids']+english_masked\n",
    "    out[\"label\"] = german_labels+label\n",
    "    return out\n",
    "\n",
    "def collate_mlm_data(features):  \n",
    "    \"\"\"\n",
    "    collate function used in data processing\n",
    "    \"\"\"\n",
    "    batch = {}\n",
    "    \n",
    "    lst_input_ids = [f[\"input_ids\"] for f in features]\n",
    "    lst_labels = [f[\"label\"] for f in features]\n",
    "    lst_attn_mask = []\n",
    "    for i in range(len(lst_input_ids)):\n",
    "        m = len(lst_input_ids[i])\n",
    "        lst_input_ids[i].extend([tokenizer.pad_token_id]*(max_len-m))\n",
    "        lst_labels[i].extend([-100]*(max_len-m))\n",
    "        attention = [1]*m\n",
    "        attention.extend([0]*(max_len-m))\n",
    "        lst_attn_mask.append(attention)\n",
    "\n",
    "    batch[\"input_ids\"] = torch.tensor(lst_input_ids, dtype=torch.long)\n",
    "    batch[\"attn_mask\"] = torch.tensor(lst_attn_mask, dtype=torch.long)\n",
    "    batch[\"labels\"] = torch.tensor(lst_labels, dtype=torch.long)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55927974-84bb-41fe-9cdd-abc5744b68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yake parameter to identify the important keywords\n",
    "top_n = 20\n",
    "language = \"en\"\n",
    "max_ngram_size = 1\n",
    "deduplication_threshold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=top_n, features=None)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bfcdd8f-7147-498e-8af3-e409bf8c3b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1754716 [00:00<?, ? examples/s]\n",
      "Map:   0%|          | 0/194969 [00:00<?, ? examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(prepare_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61f675fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/7 shards):   0%|          | 0/1754716 [00:00<?, ? examples/s]\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/194969 [00:00<?, ? examples/s]\n"
     ]
    }
   ],
   "source": [
    "# saving dataset\n",
    "tokenized_dataset.save_to_disk(\"tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "521d81c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/1754716 [00:00<?, ? examples/s]\n",
      "Filter:   0%|          | 0/194969 [00:00<?, ? examples/s]\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/50000 [00:00<?, ? examples/s]\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]\n"
     ]
    }
   ],
   "source": [
    "# extracting smaller batch from the tokenized dataset,\n",
    "# smaller dataset is used to test the theory, once it shows the significance, we use larger dataset\n",
    "train = filtered_tokenized_dataset['train'].filter(lambda example, indice: indice<50000, with_indices=True)\n",
    "test = filtered_tokenized_dataset['test'].filter(lambda example, indice: indice<5000, with_indices=True)\n",
    "tokenized_dataset_sample_batch = DatasetDict({\"train\":train, \"test\":test})\n",
    "tokenized_dataset_sample_batch.save_to_disk(\"tokenized_dataset_sample_batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6422747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
