{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6916af7",
   "metadata": {},
   "source": [
    "# this script will be used for evaluation, it will load the model trained training notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7484ef7-a2e0-4bab-b6ef-3cc15f3edd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, RobertaForMaskedLM\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import yake\n",
    "import spacy\n",
    "import collections\n",
    "import argparse\n",
    "from scipy.stats import kendalltau, pearsonr, spearmanr\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d977a8f-d079-4c64-b0f3-4aac062b0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"trained_model/\"\n",
    "model_size = 'base'\n",
    "add_pos = True\n",
    "mask_random = False\n",
    "batch_size = 64\n",
    "m_ratio = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f208e261-b38e-4743-a7ed-0ad05f58af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(model_dir==\"roberta-base\"):\n",
    "    print(\"Evaluating with roberta-base model.\")\n",
    "    model_dir = \"roberta-base\"\n",
    "    out_dir = \"result_roberta_base\"\n",
    "    no_pre = True\n",
    "else:\n",
    "    if(not os.path.isdir(model_dir)):\n",
    "        print(\"Model Directory does not exist.\")\n",
    "        exit(0)\n",
    "    else:\n",
    "        out_dir = os.path.join(model_dir, args['out'])\n",
    "\n",
    "out_dir = args['out']\n",
    "if(not os.path.isdir(out_dir)):\n",
    "    os.mkdir(out_dir)\n",
    "dataset = \"Golden_test_set_de_en.tsv\"\n",
    "out_path = os.path.join(out_dir, 'out_label_logs.txt')\n",
    "logger = open(out_path, \"w\")\n",
    "\n",
    "#----------------------------\n",
    "\n",
    "SEED = 10\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)      \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPUs!\")\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    \n",
    "lst_pos_tags = ['NN', 'NNP', 'NNS', 'JJ', 'CD', 'VB', 'VBN', 'VBD', 'VBG', 'RB', 'VBP', 'VBZ', 'NNPS', 'JJS']\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "ROBERTA_MODEL = \"xlm-roberta-base\"\n",
    "max_len_english = 64\n",
    "top_n = 20\n",
    "language = \"en\"\n",
    "max_ngram_size = 1\n",
    "deduplication_threshold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=top_n, features=None)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"Modules Loaded\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ROBERTA_MODEL)\n",
    "model = RobertaForMaskedLM.from_pretrained(model_dir, use_safetensors = True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded\")\n",
    "\n",
    "\n",
    "#----------------------------\n",
    "\n",
    "def tokenize_sentence(txt, tokenizer):\n",
    "    \"\"\"\n",
    "    Sentence tokenizer\n",
    "    \"\"\"\n",
    "    result = tokenizer(txt, max_length=max_len_english, padding='max_length', truncation=True)\n",
    "    word_ids = result.word_ids()\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [word_ids[i] for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "def get_word_mapping(tok):\n",
    "    \"\"\"\n",
    "    once sentence is converted into token, this function maps the word id to token id\n",
    "    \"\"\"\n",
    "    word_ids = tok[\"word_ids\"].copy()\n",
    "    mapping = collections.defaultdict(list)\n",
    "    current_word_index = -1\n",
    "    current_word = None\n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is not None:\n",
    "            if word_id != current_word:\n",
    "                current_word = word_id\n",
    "                current_word_index += 1\n",
    "            mapping[current_word_index].append(idx)\n",
    "    return mapping\n",
    "\n",
    "def get_pos_tags(doc):\n",
    "    \"\"\"\n",
    "    From the sentence we get the POS tags, used in masking\n",
    "    \"\"\"\n",
    "    pos_tags = {}\n",
    "    for token in doc:\n",
    "        if(not (token.is_stop or token.is_punct or token.is_space or token.text.lower() in stop_words)):\n",
    "            if(token.tag_ in lst_pos_tags):\n",
    "                pos_tags[token.text] = token.tag_\n",
    "    return pos_tags\n",
    "\n",
    "def get_mask_phrases(txt, tok, mapping, add_pos):\n",
    "    \"\"\"\n",
    "    This function mask the phrases from the sentence\n",
    "    \"\"\"\n",
    "    prev_word = None\n",
    "    prev_id = None\n",
    "    next = False\n",
    "    if(mask_random):\n",
    "        n_sample = math.ceil(0.15*len(mapping))\n",
    "        mask = random.sample(range(len(mapping)),n_sample)\n",
    "        mask_words = []\n",
    "        for idx in mask:\n",
    "            start, end = tok.word_to_chars(idx)\n",
    "            word = txt[start:end].lower()\n",
    "            mask_words.append(word)\n",
    "    else:\n",
    "        yake_doc = txt.replace(tokenizer.eos_token, \"\")\n",
    "        yake_doc = yake_doc.replace(tokenizer.bos_token, \"\")\n",
    "        yake_doc = yake_doc.strip()\n",
    "        max_keyword = max(3, math.ceil(m_ratio*len(mapping)))\n",
    "        keywords = custom_kw_extractor.extract_keywords(yake_doc)[:max_keyword]\n",
    "        lst_kw = [kw[0].lower() for kw in keywords]\n",
    "        if(len(lst_kw)<max_keyword and add_pos):\n",
    "            n = max_keyword-len(lst_kw)\n",
    "            txt_doc = nlp(txt)\n",
    "            pos_tags = get_pos_tags(txt_doc)\n",
    "            for w in pos_tags:\n",
    "                if(w not in lst_kw):\n",
    "                    lst_kw.append(w.lower())\n",
    "                    n = n-1\n",
    "                    if(n==0):\n",
    "                        break\n",
    "\n",
    "        mask = []\n",
    "        mask_words = []\n",
    "        for idx in mapping:\n",
    "            start, end = tok.word_to_chars(idx)\n",
    "            word = txt[start:end].lower()\n",
    "            if word in lst_kw or next:\n",
    "                if prev_word is not None:\n",
    "                    mask.append(prev_id)\n",
    "                    mask_words.append(prev_word)\n",
    "                    mask.append(idx)\n",
    "                    mask_words.append(word)\n",
    "                    prev_word = None\n",
    "                else:\n",
    "                    mask.append(idx)\n",
    "                    mask_words.append(word)\n",
    "                    prev_word = None\n",
    "                if word in lst_kw:\n",
    "                    next = True\n",
    "                else:\n",
    "                    next = False\n",
    "            else:\n",
    "                prev_word = word\n",
    "                prev_id = idx\n",
    "                next = False\n",
    "    return mask, mask_words\n",
    "\n",
    "\n",
    "def get_mask_words(txt, tok, mapping, add_pos):\n",
    "    \"\"\"\n",
    "    This function mask the words from the sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    if(mask_random):\n",
    "        n_sample = math.ceil(0.15*len(mapping))\n",
    "        mask = random.sample(range(len(mapping)),n_sample)\n",
    "        mask_words = []\n",
    "        for idx in mask:\n",
    "            start, end = tok.word_to_chars(idx)\n",
    "            word = txt[start:end].lower()\n",
    "            mask_words.append(word)\n",
    "    else:\n",
    "        yake_doc = txt.replace(tokenizer.eos_token, \"\")\n",
    "        yake_doc = yake_doc.replace(tokenizer.bos_token, \"\")\n",
    "        yake_doc = yake_doc.strip()\n",
    "        max_keyword = max(3, math.ceil(m_ratio*len(mapping)))\n",
    "        keywords = custom_kw_extractor.extract_keywords(yake_doc)[:max_keyword]\n",
    "        lst_kw = [kw[0].lower() for kw in keywords]\n",
    "        if(len(lst_kw)<max_keyword and add_pos):\n",
    "            n = max_keyword-len(lst_kw)\n",
    "            txt_doc = nlp(txt)\n",
    "            pos_tags = get_pos_tags(txt_doc)\n",
    "            for w in pos_tags:\n",
    "                if(w not in lst_kw):\n",
    "                    #lst_kw.append(w)\n",
    "                    lst_kw.append(w.lower())\n",
    "                    n = n-1\n",
    "                    if(n==0):\n",
    "                        break\n",
    "\n",
    "        mask = []\n",
    "        mask_words = []\n",
    "        for idx in mapping:\n",
    "            start, end = tok.word_to_chars(idx)\n",
    "            word = txt[start:end].lower()\n",
    "            if word in lst_kw:\n",
    "                mask.append(idx)\n",
    "                mask_words.append(word)\n",
    "    return mask, mask_words\n",
    "\n",
    "def get_masked_tokens(tokenizer, tok, mapping, mask):\n",
    "    \"\"\"\n",
    "    once we get the mask word id,this function replace with masked tokens\n",
    "    \"\"\"\n",
    "    input_ids = tok[\"input_ids\"].copy()\n",
    "    labels = [-100]*len(input_ids)\n",
    "    for word_id in mask:\n",
    "        for idx in mapping[word_id]:\n",
    "            labels[idx] = input_ids[idx]\n",
    "            input_ids[idx] = tokenizer.mask_token_id\n",
    "    return input_ids, labels\n",
    "\n",
    "def evaluate(input_id, lbl, attn_mask):\n",
    "    \"\"\"\n",
    "    evaluate the each sentence\n",
    "    \"\"\"\n",
    "    b_input_ids = torch.tensor([input_id], dtype=torch.long).to(device)\n",
    "    b_labels = torch.tensor([lbl], dtype=torch.long).to(device)\n",
    "    b_attn_mask = torch.tensor([attn_mask], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs_embeds = model.roberta.embeddings.word_embeddings(b_input_ids)\n",
    "        output = model(inputs_embeds=inputs_embeds, attention_mask=b_attn_mask, labels=b_labels)\n",
    "        loss = output.loss.item()\n",
    "    return loss\n",
    "\n",
    "def calculate_score(english, german):\n",
    "    tok_english = tokenize_sentence(english, tokenizer)\n",
    "    map_english_words = get_word_mapping(tok_english)\n",
    "    mask, mask_words = get_mask_phrases(english, tok_english, map_english_words, False)\n",
    "    english_masked, label = get_masked_tokens(tokenizer, tok_english, map_english_words, mask)\n",
    "\n",
    "    tok_german = tokenize_sentence(german, tokenizer)\n",
    "    german_labels = [-100]*len(tok_german['input_ids'])\n",
    "    input_id = tok_german['input_ids']+english_masked\n",
    "    label = german_labels+label\n",
    "    \n",
    "    attn_mask = [1]*(len(input_id))\n",
    "    attn_mask.extend([0]*(max_len-len(input_id)))\n",
    "    \n",
    "    input_id.extend([tokenizer.pad_token_id]*(max_len - len(input_id)))\n",
    "    label.extend([-100]*(max_len - len(label)))\n",
    "    logger.write(f\"mask_words: {mask_words}\\n\")\n",
    "    logger.write(f\"mask: {mask}\\n\")\n",
    "    \n",
    "    score = evaluate(input_id, label, attn_mask)\n",
    "    return round(score, 4), mask_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eed9c8bb-89c0-431e-843b-48a83ebe96e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading test set\n",
    "dataset = 'Golden_test_set_de_en.tsv'\n",
    "df_test_set = pd.read_csv(dataset, sep = \"\\t\")\n",
    "df_test_set = df_test_set.rename(columns = {\"wmt-z:seg\" : \"HUMAN_score\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e55bf5f6-3742-470a-be6c-8f38aa4e6b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17203\n"
     ]
    }
   ],
   "source": [
    "# inferencing for each records\n",
    "scores = []\n",
    "masked_words_list = []\n",
    "for i, row in df_test_set.iterrows():\n",
    "    german = row['source']\n",
    "    english = row['output']\n",
    "    score, masked_words_ = calculate_score(english, german)\n",
    "    scores.append(score)\n",
    "    masked_words_list.append(masked_words_)\n",
    "    print(i, end = \"\\r\")\n",
    "df_test_set['dial-m_score'] = scores\n",
    "df_test_set['mask_words'] = masked_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db46153-2749-4898-9c4e-b5f13a84a68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendall tau-c: (-0.3249, 0)\n",
      "Kendall tau-b: (-0.3378, 0)\n",
      "Correlation between GT and score: Pearson = (-0.4534, 0.0), Spearman = (-0.4509, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# evaluation: xlm-roberta-base + lr=5e-5 + phrase masking + separate embeddings\n",
    "x = df_test_set['HUMAN_score']\n",
    "y = df_test_set['dial-m_score']\n",
    "pearson_corr, pearson_p_val = pearsonr(x, y)\n",
    "spearman_corr, spearman_p_val = spearmanr(x, y)\n",
    "tau_c, pval_c = kendalltau(x, y, variant='c')\n",
    "tau_b, pval_b = kendalltau(x, y, variant='b')\n",
    "print(\"Kendall tau-c:\", (round(tau_c, 4), round(pval_c, 4)))\n",
    "print(\"Kendall tau-b:\", (round(tau_b, 4), round(pval_b, 4)))\n",
    "print(f\"Correlation between GT and score: Pearson = {(round(pearson_corr, 4), round(pearson_p_val, 4))}, Spearman = {(round(spearman_corr, 4), round(spearman_p_val, 4))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ecd02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
